{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "import time\n",
    "from torch.autograd import Variable\n",
    "import torchtext\n",
    "import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torchtext.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_label = data.Field(use_vocab=True, sequential=False, \n",
    "                         lower=True, is_target=True)\n",
    "sentence = data.Field(use_vocab=True, sequential=True,  \n",
    "                     lower=True, fix_length=105,\n",
    "                     init_token = \"<s>\", eos_token = \"<\\s>\",)\n",
    "train, validation = data.TabularDataset.splits(path=\"topicclass/\",\n",
    "                            train='topicclass_train.tsv',\n",
    "                            validation='topicclass_valid.tsv',\n",
    "                            skip_header=False, format='tsv',\n",
    "                            fields = [\n",
    "                                ('class_label', class_label), \n",
    "                                ('sentence', sentence)\n",
    "                            ]\n",
    "                           )\n",
    "test = data.TabularDataset(path=\"topicclass/topicclass_test.tsv\",\n",
    "                          skip_header=True, format='tsv',\n",
    "                          fields = [\n",
    "                              ('class_label', None),\n",
    "                              ('sentence', sentence)\n",
    "                          ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_label.build_vocab(train, validation)\n",
    "sentence.build_vocab(train, validation, test, min_freq = 5, vectors = 'glove.840B.300d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "        \"batch_size\": 4096,\n",
    "        \"embedding_size\": 300,\n",
    "        \"sentence_length\": 105,\n",
    "        \"vocab_size\": len(sentence.vocab),\n",
    "        \"n_classes\": len(class_label.vocab),\n",
    "        \"filter_sizes\": [3, 5, 7],\n",
    "        \"filter_num\": [100, 100, 100],\n",
    "        \"PreTrained\" : sentence.vocab.vectors\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, **params):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.batch_size = params[\"batch_size\"]\n",
    "        self.sentence_length = params[\"sentence_length\"]\n",
    "        self.embedding_size = params[\"embedding_size\"]\n",
    "        self.vocab_size = params[\"vocab_size\"]\n",
    "        self.n_classes = params[\"n_classes\"]\n",
    "        self.filter_sizes = params[\"filter_sizes\"]\n",
    "        self.filter_num = params[\"filter_num\"]\n",
    "        self.PreTrained = params[\"PreTrained\"]\n",
    "        self.in_channel = 1\n",
    "  \n",
    "        \n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_size)\n",
    "        self.embedding.weight.data.copy_(self.PreTrained)\n",
    "\n",
    "        \n",
    "        self.conv0 = nn.Conv1d(self.in_channel, self.filter_num[0], self.embedding_size * self.filter_sizes[0], stride=self.embedding_size)\n",
    "        self.conv1 = nn.Conv1d(self.in_channel, self.filter_num[1], self.embedding_size * self.filter_sizes[1], stride=self.embedding_size)\n",
    "        self.conv2 = nn.Conv1d(self.in_channel, self.filter_num[2], self.embedding_size * self.filter_sizes[2], stride=self.embedding_size)\n",
    "        \n",
    "        self.linear1 = nn.Linear(sum(self.filter_num), sum(self.filter_num))\n",
    "        self.linear2 = nn.Linear(sum(self.filter_num), self.n_classes)\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        x = self.embedding(inp).view(-1, 1, self.embedding_size * self.sentence_length\n",
    "\n",
    "\n",
    "        \n",
    "        x1 = F.max_pool1d(F.relu(self.conv0(x)), self.sentence_length - self.filter_sizes[0] + 1).view(-1, self.filter_num[0])\n",
    "                      \n",
    "        x2 = F.max_pool1d(F.relu(self.conv1(x)), self.sentence_length - self.filter_sizes[1] + 1).view(-1, self.filter_num[1])\n",
    "                      \n",
    "        x3 = F.max_pool1d(F.relu(self.conv2(x)), self.sentence_length - self.filter_sizes[2] + 1).view(-1, self.filter_num[2]) \n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        x = torch.cat([x1,x2,x3], 1)\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = F.relu(self.linear2(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN(**params) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "type = torch.LongTensor\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "if use_cuda:\n",
    "    type = torch.cuda.LongTensor\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr = 8e-4, weight_decay=8e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = data.BucketIterator(\n",
    " (train), \n",
    " batch_size=4096,\n",
    " device=torch.device('cuda'),\n",
    " shuffle = True,\n",
    " repeat=False \n",
    ")\n",
    "\n",
    "valid_iter = data.BucketIterator(\n",
    " (validation), \n",
    " batch_size=4096,\n",
    " device=torch.device('cuda'),\n",
    " shuffle = False,\n",
    " repeat=False \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 1.0660, Validation Loss: 0.7238, Validation acc: 81.8040\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-b88f645175e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1,21):\n",
    "    train_loss = 0\n",
    "    for batch in train_iter: \n",
    "        model.train()\n",
    "        opt.zero_grad()\n",
    "        text, targets = batch.sentence, batch.class_label\n",
    "        text = torch.t(text)\n",
    "        prediction = model(text)\n",
    "        loss = criterion(prediction.view(-1,len(class_label.vocab)),targets.view(-1))\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    " \n",
    "        train_loss += loss.data.item() * text.size(0)\n",
    "        model.eval()\n",
    "         \n",
    "    train_loss /= len(train.examples)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        model.eval() \n",
    "        for batch in valid_iter:\n",
    "            text, targets = batch.sentence, batch.class_label\n",
    "            text = torch.t(text)\n",
    "            prediction = model(text)\n",
    "            loss = criterion(prediction,targets)\n",
    "            val_loss += loss.data.item() * text.size(0)\n",
    "            val_acc = float(100*(prediction.argmax(dim = 1) == targets).sum())/len(validation.examples)\n",
    "        val_loss /= len(validation.examples)\n",
    "    if ((train_loss < 0.8) & (val_loss < 0.73) & ( val_acc > 82.5)):\n",
    "        torch.save(model, \"model_{}\".format(epoch))\n",
    "        \n",
    "    print('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}, Validation acc: {:.4f}'.format(epoch, train_loss, val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the saved model was trained with a little different formatting of the code\n",
    "\n",
    "params = {\n",
    "        \"BATCH_SIZE\": 4096,\n",
    "        \"WORD_DIM\": 300,\n",
    "        \"MAX_SENT_LEN\": 105,\n",
    "        \"VOCAB_SIZE\": len(sentence.vocab),\n",
    "        \"CLASS_SIZE\": len(class_label.vocab),\n",
    "        \"FILTERS\": [3, 5, 7],\n",
    "        \"FILTER_NUM\": [100, 100, 100],\n",
    "        \"PreTrained\" : sentence.vocab.vectors\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, **params):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.BATCH_SIZE = params[\"BATCH_SIZE\"]\n",
    "        self.MAX_SENT_LEN = params[\"MAX_SENT_LEN\"]\n",
    "        self.WORD_DIM = params[\"WORD_DIM\"]\n",
    "        self.VOCAB_SIZE = params[\"VOCAB_SIZE\"]\n",
    "        self.CLASS_SIZE = params[\"CLASS_SIZE\"]\n",
    "        self.FILTERS = params[\"FILTERS\"]\n",
    "        self.FILTER_NUM = params[\"FILTER_NUM\"]\n",
    "        self.PreTrained = params[\"PreTrained\"]\n",
    "        self.IN_CHANNEL = 1\n",
    "    \n",
    "        \n",
    "        self.embedding = nn.Embedding(self.VOCAB_SIZE, self.WORD_DIM)\n",
    "        self.embedding.weight.data.copy_(self.PreTrained)\n",
    "\n",
    "        \n",
    "        self.conv0 = nn.Conv1d(self.IN_CHANNEL, self.FILTER_NUM[0], self.WORD_DIM * self.FILTERS[0], stride=self.WORD_DIM)\n",
    "        self.conv1 = nn.Conv1d(self.IN_CHANNEL, self.FILTER_NUM[1], self.WORD_DIM * self.FILTERS[1], stride=self.WORD_DIM)\n",
    "        self.conv2 = nn.Conv1d(self.IN_CHANNEL, self.FILTER_NUM[2], self.WORD_DIM * self.FILTERS[2], stride=self.WORD_DIM)\n",
    "        \n",
    "        self.linear1 = nn.Linear(sum(self.FILTER_NUM), sum(self.FILTER_NUM))\n",
    "        self.linear2 = nn.Linear(sum(self.FILTER_NUM), self.CLASS_SIZE)\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        x = self.embedding(inp).view(-1, 1, self.WORD_DIM * self.MAX_SENT_LEN)\n",
    "        #x2 = self.embedding2(inp).view(-1, 1, self.WORD_DIM * self.MAX_SENT_LEN)\n",
    "        #x = torch.cat((x, x2), 1)\n",
    "\n",
    "\n",
    "        \n",
    "        x1 = F.max_pool1d(F.relu(self.conv0(x)), self.MAX_SENT_LEN - self.FILTERS[0] + 1).view(-1, self.FILTER_NUM[0])\n",
    "                      \n",
    "        x2 = F.max_pool1d(F.relu(self.conv1(x)), self.MAX_SENT_LEN - self.FILTERS[1] + 1).view(-1, self.FILTER_NUM[1])\n",
    "                      \n",
    "        x3 = F.max_pool1d(F.relu(self.conv2(x)), self.MAX_SENT_LEN - self.FILTERS[2] + 1).view(-1, self.FILTER_NUM[2]) \n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        x = torch.cat([x1,x2,x3], 1)\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = F.relu(self.linear2(x))\n",
    "        \n",
    "        return x\n",
    "        \n",
    "\n",
    "model = torch.load(\"model_15\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.5626, Validation acc: 85.3810\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    val_loss =0.0\n",
    "    model.eval()\n",
    "    for batch in valid_iter:\n",
    "        text, targets = batch.sentence, batch.class_label\n",
    "        text = torch.t(text)\n",
    "        prediction = model(text)\n",
    "        loss = criterion(prediction,targets)\n",
    "        val_loss += loss.data.item() * text.size(0)\n",
    "        val_acc = float(100*(prediction.argmax(dim = 1) == targets).sum())/len(validation.examples)\n",
    "    val_loss /= len(validation.examples)\n",
    "    print('Validation Loss: {:.4f}, Validation acc: {:.4f}'.format(val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_iter = data.BucketIterator(\n",
    " (test), # we pass in the datasets we want the iterator to draw data from\n",
    " batch_size=4096,\n",
    " device=torch.device('cuda'),\n",
    " shuffle = False,# if you want to use the GPU, specify the GPU number here\n",
    " repeat=False # we pass repeat=False because we want to wrap this Iterator layer.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_label =np.array([None]*643)\n",
    "test_label =np.array([None]*696)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_label[np.array(prediction.argmax(dim =1)) == 1] = 'music '\n",
    "val_label[np.array(prediction.argmax(dim =1)) == 2] = 'sports and recreation '\n",
    "val_label[np.array(prediction.argmax(dim =1)) == 3] = 'natural sciences '\n",
    "val_label[np.array(prediction.argmax(dim =1)) == 4] = 'warfare '\n",
    "val_label[np.array(prediction.argmax(dim =1)) == 5] = 'media and drama '\n",
    "val_label[np.array(prediction.argmax(dim =1)) == 6] = 'social sciences and society '\n",
    "val_label[np.array(prediction.argmax(dim =1)) == 7] = 'history '\n",
    "val_label[np.array(prediction.argmax(dim =1)) == 8] = 'engineering and technology '\n",
    "val_label[np.array(prediction.argmax(dim =1)) == 9] = 'geography and places '\n",
    "val_label[np.array(prediction.argmax(dim =1)) == 10] = 'video games '\n",
    "val_label[np.array(prediction.argmax(dim =1)) == 11] = 'art and architecture '\n",
    "val_label[np.array(prediction.argmax(dim =1)) == 12] = 'language and literature '\n",
    "val_label[np.array(prediction.argmax(dim =1)) == 13] = 'philosophy and religion '\n",
    "val_label[np.array(prediction.argmax(dim =1)) == 14] = 'agriculture, food and drink '\n",
    "val_label[np.array(prediction.argmax(dim =1)) == 15] = 'miscellaneous '\n",
    "val_label[np.array(prediction.argmax(dim =1)) == 16] = 'mathematics '\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for batch in test_iter:\n",
    "        text = batch.sentence\n",
    "        text = torch.t(text)\n",
    "        prediction = model(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label[np.array(prediction.argmax(dim =1)) == 1] = 'music '\n",
    "test_label[np.array(prediction.argmax(dim =1)) == 2] = 'sports and recreation '\n",
    "test_label[np.array(prediction.argmax(dim =1)) == 3] = 'natural sciences '\n",
    "test_label[np.array(prediction.argmax(dim =1)) == 4] = 'warfare '\n",
    "test_label[np.array(prediction.argmax(dim =1)) == 5] = 'media and drama '\n",
    "test_label[np.array(prediction.argmax(dim =1)) == 6] = 'social sciences and society '\n",
    "test_label[np.array(prediction.argmax(dim =1)) == 7] = 'history '\n",
    "test_label[np.array(prediction.argmax(dim =1)) == 8] = 'engineering and technology '\n",
    "test_label[np.array(prediction.argmax(dim =1)) == 9] = 'geography and places '\n",
    "test_label[np.array(prediction.argmax(dim =1)) == 10] = 'video games '\n",
    "test_label[np.array(prediction.argmax(dim =1)) == 11] = 'art and architecture '\n",
    "test_label[np.array(prediction.argmax(dim =1)) == 12] = 'language and literature '\n",
    "test_label[np.array(prediction.argmax(dim =1)) == 13] = 'philosophy and religion '\n",
    "test_label[np.array(prediction.argmax(dim =1)) == 14] = 'agriculture, food and drink '\n",
    "test_label[np.array(prediction.argmax(dim =1)) == 15] = 'miscellaneous '\n",
    "test_label[np.array(prediction.argmax(dim =1)) == 16] = 'mathematics '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"validation_labels\", val_label, newline=\"\\n\", fmt = \"%s\")\n",
    "np.savetxt(\"test_labels\", test_label, newline=\"\\n\", fmt = \"%s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
